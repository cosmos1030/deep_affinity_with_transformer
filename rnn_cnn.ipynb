{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sps = torch.load('data/train_sps.ids76.pt')\n",
    "train_smile = torch.load('data/train_smile.ids68.pt')\n",
    "train_ic50 = torch.load('data/train_ic50_log.pt')\n",
    "\n",
    "test_sps = torch.load('data/test_sps.ids76.pt')\n",
    "test_smile = torch.load('data/test_smile.ids68.pt')\n",
    "test_ic50 = torch.load('data/test_ic50_log.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([263583, 152])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnCnnModel(nn.Module):\n",
    "    def __init__(self, vocab_size_protein, vocab_size_compound, GRU_size_prot, GRU_size_drug, U_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.GRU_size_prot = GRU_size_prot\n",
    "        self.GRU_size_drug = GRU_size_drug\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Protein path\n",
    "        self.prot_embedding = nn.Embedding(vocab_size_protein, GRU_size_prot)\n",
    "        self.prot_gru_1 = nn.GRU(GRU_size_prot, GRU_size_prot, batch_first=True)\n",
    "        self.prot_gru_2 = nn.GRU(GRU_size_prot, GRU_size_prot, batch_first=True)\n",
    "        \n",
    "        # Drug path\n",
    "        self.drug_embedding = nn.Embedding(vocab_size_compound, GRU_size_drug)\n",
    "        self.drug_gru_1 = nn.GRU(GRU_size_drug, GRU_size_drug, batch_first=True)\n",
    "        self.drug_gru_2 = nn.GRU(GRU_size_drug, GRU_size_drug, batch_first=True)\n",
    "        \n",
    "        # Attention mechanism components\n",
    "        self.W_attn = nn.Parameter(torch.Tensor(GRU_size_prot, GRU_size_drug))\n",
    "        self.b_attn = nn.Parameter(torch.Tensor(1))\n",
    "        self.U_prot = nn.Parameter(torch.Tensor(U_size, GRU_size_prot))\n",
    "        self.U_drug = nn.Parameter(torch.Tensor(U_size, GRU_size_drug))\n",
    "        self.B = nn.Parameter(torch.Tensor(U_size))\n",
    "        \n",
    "        # Convolution and fully connected layers\n",
    "        self.conv1 = nn.Conv1d(U_size, 64, kernel_size=4, stride=2)\n",
    "        self.fc1 = nn.Linear(64*32, 600)  # Adjust the input features according to the output of conv layers\n",
    "        self.fc2 = nn.Linear(600, 300)\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "\n",
    "    def attention_mechanism(self, prot_out_2, drug_out_2):\n",
    "        # Assuming prot_out_2 and drug_out_2 are the outputs from the last GRU layer for protein and drug respectively\n",
    "        \n",
    "        # Compute VU term (dot product + addition of bias, with non-linearity)\n",
    "        # V is prot_out_2 transformed by W_attn, and U is drug_out_2 directly used\n",
    "        V = torch.matmul(prot_out_2, self.W_attn)  # [batch_size, seq_len_prot, GRU_size_drug]\n",
    "        VU = torch.tanh(torch.matmul(V, drug_out_2.transpose(1, 2)) + self.b_attn)  # [batch_size, seq_len_prot, seq_len_drug]\n",
    "        \n",
    "        # Flatten VU to compute softmax\n",
    "        VU_flat = VU.view(-1, VU.size(-1))  # Flatten to [batch_size * seq_len_prot, seq_len_drug]\n",
    "        alphas_flat = F.softmax(VU_flat, dim=1)\n",
    "        alphas = alphas_flat.view(VU.size())  # Reshape back to [batch_size, seq_len_prot, seq_len_drug]\n",
    "        \n",
    "        # Compute context vector as weighted sum\n",
    "        context_vector = torch.bmm(alphas, drug_out_2)  # [batch_size, seq_len_prot, GRU_size_drug]\n",
    "        \n",
    "        # Compute final attention vector (Attn) by combining U_prot, U_drug, and B terms\n",
    "        Attn = torch.zeros(self.batch_size, self.U_prot.size(0), device=prot_out_2.device)  # Initialize Attn\n",
    "        for i in range(context_vector.size(1)):  # Iterate over seq_len_prot\n",
    "            temp = torch.matmul(context_vector[:, i, :], self.U_drug) + torch.matmul(prot_out_2[:, i, :], self.U_prot) + self.B\n",
    "            Attn += temp * alphas[:, i, :].unsqueeze(2)  # Weight by alphas\n",
    "        \n",
    "        return Attn\n",
    "\n",
    "    def forward(self, prot_data, drug_data):\n",
    "        # Embedding and GRU for protein\n",
    "        prot_emb = self.prot_embedding(prot_data)\n",
    "        prot_out_1, _ = self.prot_gru_1(prot_emb)\n",
    "        prot_out_2, _ = self.prot_gru_2(prot_out_1)\n",
    "        \n",
    "        # Embedding and GRU for drug\n",
    "        drug_emb = self.drug_embedding(drug_data)\n",
    "        drug_out_1, _ = self.drug_gru_1(drug_emb)\n",
    "        drug_out_2, _ = self.drug_gru_2(drug_out_1)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        Attn = self.attention_mechanism(prot_out_2, drug_out_2)\n",
    "        \n",
    "        # Assuming Attn is appropriately resized or pooled for the convolutional layers\n",
    "        Attn = Attn.unsqueeze(1)  # Add channel dimension for conv1d\n",
    "        conv_out = F.relu(self.conv1(Attn))\n",
    "        conv_out = conv_out.view(conv_out.size(0), -1)  # Flatten for FC layers\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc_out = F.relu(self.fc1(conv_out))\n",
    "        fc_out = F.dropout(fc_out, 0.8, training=self.training)\n",
    "        fc_out = F.relu(self.fc2(fc_out))\n",
    "        fc_out = F.dropout(fc_out, 0.8, training=self.training)\n",
    "        output = self.fc3(fc_out)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "protein_vocab_size = 76 # 토큰 어휘집의 어휘 개수\n",
    "compound_vocab_size = 68 # 토큰 어휘집의 어휘 개수\n",
    "protein_seq_length = 152 # 한 문장의 토큰 수\n",
    "compound_seq_length = 100 # 한 문장의 토큰 수\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "batch_size = 64\n",
    "U_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RnnCnnModel(\n",
       "  (prot_embedding): Embedding(76, 256)\n",
       "  (prot_gru_1): GRU(256, 256, batch_first=True)\n",
       "  (prot_gru_2): GRU(256, 256, batch_first=True)\n",
       "  (drug_embedding): Embedding(68, 256)\n",
       "  (drug_gru_1): GRU(256, 256, batch_first=True)\n",
       "  (drug_gru_2): GRU(256, 256, batch_first=True)\n",
       "  (conv1): Conv1d(256, 64, kernel_size=(4,), stride=(2,))\n",
       "  (fc1): Linear(in_features=2048, out_features=600, bias=True)\n",
       "  (fc2): Linear(in_features=600, out_features=300, bias=True)\n",
       "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RnnCnnModel(protein_vocab_size, compound_vocab_size, embedding_dim, embedding_dim, U_size, batch_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/4119 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m log_ic50 \u001b[38;5;241m=\u001b[39m log_ic50\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 그라디언트 초기화\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# drug sequence에서 look ahead mask는 필요가 없음\u001b[39;00m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), log_ic50\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/cuda117/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 67\u001b[0m, in \u001b[0;36mRnnCnnModel.forward\u001b[0;34m(self, prot_data, drug_data)\u001b[0m\n\u001b[1;32m     64\u001b[0m drug_out_2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrug_gru_2(drug_out_1)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Attention mechanism\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m Attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mechanism\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprot_out_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_out_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Assuming Attn is appropriately resized or pooled for the convolutional layers\u001b[39;00m\n\u001b[1;32m     70\u001b[0m Attn \u001b[38;5;241m=\u001b[39m Attn\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add channel dimension for conv1d\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 51\u001b[0m, in \u001b[0;36mRnnCnnModel.attention_mechanism\u001b[0;34m(self, prot_out_2, drug_out_2)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(context_vector\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)):  \u001b[38;5;66;03m# Iterate over seq_len_prot\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     temp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(context_vector[:, i, :], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_drug) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(prot_out_2[:, i, :], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mU_prot) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB\n\u001b[0;32m---> 51\u001b[0m     Attn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtemp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malphas\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Weight by alphas\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Attn\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "shuffle = True\n",
    "\n",
    "dataset = TensorDataset(train_sps, train_smile, train_ic50)\n",
    "data_loader = DataLoader(dataset, batch_size= batch_size, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# 훈련 시작\n",
    "for epoch in range(num_epochs):\n",
    "    model.to(device)\n",
    "    model.train()  # 모델을 훈련 모드로 설정\n",
    "    total_loss = 0.0  # 에폭별 총 손실을 추적\n",
    "    \n",
    "    # tqdm을 사용하여 진행 상황 막대 표시\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, (sps, smile, log_ic50) in progress_bar:\n",
    "        sps = sps.to(device)\n",
    "        smile = smile.to(device)\n",
    "        log_ic50 = log_ic50.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 그라디언트 초기화\n",
    "        \n",
    "        output = model(sps, smile) # drug sequence에서 look ahead mask는 필요가 없음\n",
    "        \n",
    "        loss = criterion(output.squeeze(), log_ic50.float())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 진행률 막대에 현재 평균 손실 표시\n",
    "        progress_bar.set_postfix({'avg_loss': total_loss / (batch_idx + 1)})\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda117",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
